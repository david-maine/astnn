{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "    \n",
    "import pandas as pd\n",
    "import random\n",
    "import torch\n",
    "import time\n",
    "import numpy as np\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "from model import BatchProgramClassifier\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import re\n",
    "\n",
    "from pycparser.c_ast import TypeDecl, ArrayDecl\n",
    "from pycparser import c_ast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "root = '/home/david/projects/university/astnn/data/'\n",
    "\n",
    "word2vec = Word2Vec.load(root+\"train/embedding/node_w2v_128\").wv\n",
    "embeddings = np.zeros((word2vec.vectors.shape[0] + 1, word2vec.vectors.shape[1]), dtype=\"float32\")\n",
    "embeddings[:word2vec.vectors.shape[0]] = word2vec.vectors\n",
    "\n",
    "HIDDEN_DIM = 100\n",
    "ENCODE_DIM = 128\n",
    "LABELS = 104\n",
    "EPOCHS = 15\n",
    "BATCH_SIZE = 1\n",
    "USE_GPU = False\n",
    "MAX_TOKENS = word2vec.vectors.shape[0]\n",
    "EMBEDDING_DIM = word2vec.vectors.shape[1]\n",
    "\n",
    "model = BatchProgramClassifier(EMBEDDING_DIM,HIDDEN_DIM,MAX_TOKENS+1,ENCODE_DIM,LABELS,BATCH_SIZE,\n",
    "                               USE_GPU, embeddings)\n",
    "model.load_state_dict(torch.load(\"/home/david/projects/university/astnn/model.pt\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec = Word2Vec.load('/home/david/projects/university/astnn/data/train/embedding/node_w2v_128').wv\n",
    "vocab = word2vec.vocab\n",
    "\n",
    "ast_data = pd.read_pickle(root+'test/test_.pkl')\n",
    "block_data = pd.read_pickle(root+'test/blocks.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Allowed var names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "leaf_embed = nn.Sequential(\n",
    "    model._modules['encoder']._modules['embedding'],\n",
    "    model._modules['encoder']._modules['W_c']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# words we wont allow as variable names\n",
    "reserved_words = [\n",
    "    'auto',\n",
    "    'break',\n",
    "    'case',\n",
    "    'char',\n",
    "    'const',\n",
    "    'continue',\n",
    "    'default',\n",
    "    'do',\n",
    "    'int',\n",
    "    'long',\n",
    "    'register',\n",
    "    'return',\n",
    "    'short',\n",
    "    'sizeof',\n",
    "    'static',\n",
    "    'struct',\n",
    "    'switch',\n",
    "    'typedef',\n",
    "    'union',\n",
    "    'unsigned',\n",
    "    'void',\n",
    "    'volatile',\n",
    "    'while',\n",
    "    'double',\n",
    "    'else',\n",
    "    'enum',\n",
    "    'extern',\n",
    "    'float',\n",
    "    'for',\n",
    "    'goto',\n",
    "    'if',\n",
    "    'printf',\n",
    "    'scanf',\n",
    "    'cos',\n",
    "    'malloc'\n",
    "]\n",
    "\n",
    "\n",
    "def allowed_variable(var):\n",
    "    pattern = re.compile(\"([a-z]|[A-Z]|_)+([a-z]|[A-Z]|[0-9]|_)*$\")\n",
    "    if (var not in reserved_words) and pattern.match(var):\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "allowed_variable('scanf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_map = {}\n",
    "\n",
    "for index in range(len(vocab)):\n",
    "    if allowed_variable(word2vec.index2word[index]):\n",
    "        embedding_map[index] = leaf_embed(torch.tensor(index)).detach().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Var replace functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_index(node, old_i, new_i):\n",
    "    i = node[0]\n",
    "    if i == old_i:\n",
    "        result = [new_i]\n",
    "    else:\n",
    "        result = [i]\n",
    "    children = node[1:]\n",
    "    for child in children:\n",
    "        result.append(replace_index(child, old_i, new_i))\n",
    "    return result\n",
    "\n",
    "def replace_var(x, old_i, new_i):\n",
    "    mod_blocks = []\n",
    "    for block in x:\n",
    "        mod_blocks.append(replace_index(block, old_i, new_i))\n",
    "\n",
    "    return mod_blocks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Closest Var functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def l1_norm(a, b):\n",
    "    return np.linalg.norm(a-b, 1)\n",
    "\n",
    "def l2_norm(a, b):\n",
    "    return np.linalg.norm(a-b)\n",
    "\n",
    "def cos_sim(a, b):\n",
    "    return np.inner(a, b) / (np.linalg.norm(a) * np.linalg.norm(b))\n",
    "\n",
    "def closest_index(embedding, embedding_map, metric):\n",
    "    embedding = embedding.detach().numpy()\n",
    "    closest_i = list(embedding_map.keys())[0]\n",
    "    closest_dist = metric(embedding_map[closest_i], embedding)\n",
    "    for i, e in embedding_map.items():\n",
    "        d = metric(embedding_map[i], embedding)\n",
    "        if d < closest_dist:\n",
    "            closest_dist = d\n",
    "            closest_i = i\n",
    "    return closest_i\n",
    "\n",
    "def normalize(v):\n",
    "    norm = np.linalg.norm(v)\n",
    "    if norm == 0:\n",
    "        return v\n",
    "    return v / norm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grad locating functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embedding(indices, node_list):\n",
    "    '''\n",
    "    get the embeddings at the index positions in postorder traversal.\n",
    "    '''\n",
    "    res = []\n",
    "    c = 0\n",
    "    for i in range(node_list.size(0)):\n",
    "        if not np.all(node_list[i].detach().numpy() == 0):\n",
    "            if c in indices:\n",
    "                res.append(node_list[i])\n",
    "            c += 1\n",
    "    return res\n",
    "\n",
    "def post_order_loc(node, var, res, counter):\n",
    "    '''\n",
    "    \n",
    "    '''\n",
    "    index = node[0]\n",
    "    children = node[1:]\n",
    "    for child in children:\n",
    "        res, counter = post_order_loc(child, var, res, counter)\n",
    "    if var == index and (not children):\n",
    "        res.append(counter) \n",
    "#         print(counter, word2vec.index2word[index])\n",
    "    counter += 1\n",
    "    return res, counter\n",
    "\n",
    "def get_grad(x, var_index, node_list):\n",
    "    grads = []\n",
    "    for i, block in enumerate(x):\n",
    "        indices, _ = post_order_loc(block, var_index, [], 0)\n",
    "        grads += get_embedding(indices, node_list.grad[:, i, :])\n",
    "        try:\n",
    "            node_embedding = get_embedding(indices, node_list[:, i, :])[0]\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "    if len(grads) < 1:\n",
    "        return None, None\n",
    "    grad = torch.stack(grads).sum(dim=0)\n",
    "    return grad, node_embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Var name finder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class declarationFinder(c_ast.NodeVisitor):\n",
    "    def __init__(self):\n",
    "        self.names = set()\n",
    "    \n",
    "    def visit_Decl(self, node):\n",
    "        if type(node.type) in [TypeDecl, ArrayDecl] :\n",
    "            self.names.add(node.name)\n",
    "\n",
    "def get_var_names(ast):\n",
    "    declaration_finder = declarationFinder()\n",
    "    declaration_finder.visit(ast)\n",
    "    return declaration_finder.names\n",
    "    \n",
    "# get_var_names(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FGSM\n",
    "\n",
    "with vars ordered and early exit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def gradient_method(x, n_list, var, epsilon, metric):\n",
    "\n",
    "#     orig_index = vocab[var].index if var in vocab else MAX_TOKEN\n",
    "\n",
    "#     grad, node_embedding = get_grad(x, orig_index, n_list)\n",
    "#     if grad is None:\n",
    "# #         print(\"no leaf occurences\")\n",
    "#         return None\n",
    "\n",
    "#     v = node_embedding.detach().numpy()\n",
    "#     g = torch.sign(grad).detach().numpy()\n",
    "    \n",
    "\n",
    "#     v = v + epsilon * g\n",
    "#     # get the closest emebedding from our map\n",
    "#     i = closest_index(v, sampled_embedding_map, metric)\n",
    "# #         print(\"orig name:\", word2vec.index2word[orig_index], \"; new name:\", word2vec.index2word[i])\n",
    "#     if i != orig_index:\n",
    "#         return replace_var(x, orig_index, i)\n",
    "#     else:\n",
    "#         return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_TOKEN = word2vec.vectors.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import datetime\n",
    "\n",
    "def evaluate(epsilon, limit = None, sort_vars = True):\n",
    "    ast_count = 0\n",
    "    var_count = 0\n",
    "\n",
    "    ast_total = 0\n",
    "    var_total = 0\n",
    "\n",
    "    start = time.time()\n",
    "    for code_id in block_data['id'].tolist():\n",
    "    #     print(code_id)\n",
    "        x, ast = block_data['code'][code_id], ast_data['code'][code_id]\n",
    "\n",
    "        _, orig_pred = torch.max(model([x]).data, 1)\n",
    "        orig_pred = orig_pred.item()\n",
    "\n",
    "        # get the grad\n",
    "        loss_function = torch.nn.CrossEntropyLoss()\n",
    "        labels = torch.LongTensor([orig_pred])\n",
    "        output = model([x])\n",
    "        loss = loss_function(output, Variable(labels))\n",
    "        loss.backward()\n",
    "        n_list = model._modules['encoder'].node_list\n",
    "\n",
    "        var_names = get_var_names(ast)\n",
    "        success = False\n",
    "        \n",
    "        var_weighted = []\n",
    "        for var in list(var_names):\n",
    "            \n",
    "            orig_index = vocab[var].index if var in vocab else MAX_TOKEN\n",
    "            grad, node_embedding = get_grad(x, orig_index, n_list)\n",
    "            if grad is not None:\n",
    "                h = abs((grad @ torch.sign(grad)).item())\n",
    "                var_weighted.append( (h, grad, node_embedding) )\n",
    "            \n",
    "        if sort_vars:\n",
    "            var_weighted = sorted(var_weighted, key=lambda x: x[0], reverse = True)\n",
    "        \n",
    "        for h, grad, node_embedding in var_weighted:\n",
    "            \n",
    "            v = node_embedding\n",
    "            g = torch.sign(grad)\n",
    "\n",
    "\n",
    "            v = v + epsilon * g\n",
    "            # get the closest emebedding from our map\n",
    "#             i = closest_index(v, sampled_embedding_map, l2_norm)\n",
    "            i = closest_index(v, sampled_embedding_map, cos_sim)\n",
    "            if i != orig_index:\n",
    "                new_x_l2 = replace_var(x, orig_index, i)\n",
    "            else:\n",
    "                new_x_l2 = x\n",
    "            \n",
    "            if new_x_l2:\n",
    "                o = model([new_x_l2])\n",
    "                _, predicted_l2 = torch.max(o.data, 1)\n",
    "\n",
    "    #             print(orig_pred, predicted_l2.item())\n",
    "                var_total += 1\n",
    "                if orig_pred != predicted_l2.item():\n",
    "                    var_count += 1\n",
    "                    success = True\n",
    "                    break\n",
    "\n",
    "        if success:\n",
    "            ast_count += 1\n",
    "        ast_total += 1\n",
    "\n",
    "\n",
    "        if ast_total % 500 == 499:\n",
    "            eval_time = time.time() - start\n",
    "            eval_time = datetime.timedelta(seconds=eval_time)\n",
    "            print(ast_total, \";\", eval_time, \";\", ast_count / ast_total, \";\", var_count / var_total)\n",
    "    \n",
    "        if limit and limit < ast_total:\n",
    "            break\n",
    "    return (ast_count / ast_total, var_count / var_total)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample_rate = 0.2\n",
    "# sample_count = int(len(embedding_map) * sample_rate)\n",
    "# sampled_embedding_map = {key: embedding_map[key] for key in random.sample(embedding_map.keys(), sample_count)}\n",
    "\n",
    "sampled_embedding_map = embedding_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "499 ; 0:05:18.702358 ; 0.06613226452905811 ; 0.010655473038424281\n",
      "999 ; 0:11:50.678249 ; 0.06906906906906907 ; 0.01107011070110701\n",
      "1499 ; 0:18:57.640237 ; 0.07538358905937291 ; 0.012034078807241746\n",
      "1999 ; 0:26:48.284370 ; 0.07703851925962982 ; 0.012212529738302934\n",
      "2499 ; 0:36:03.751547 ; 0.07603041216486595 ; 0.011997979287698914\n",
      "2999 ; 0:45:53.149422 ; 0.07535845281760586 ; 0.011884728649558267\n",
      "3499 ; 0:55:05.345233 ; 0.07630751643326665 ; 0.012091296078253782\n",
      "3999 ; 1:03:42.916828 ; 0.07476869217304326 ; 0.011816313626304142\n",
      "4499 ; 1:10:11.879108 ; 0.07535007779506557 ; 0.011945873564028472\n",
      "4999 ; 1:17:21.407472 ; 0.07561512302460492 ; 0.011951057573745613\n",
      "5499 ; 1:24:01.683448 ; 0.07619567194035279 ; 0.012050964940032789\n",
      "5999 ; 1:30:50.509424 ; 0.07634605767627937 ; 0.012086346123396844\n",
      "6499 ; 1:37:27.652857 ; 0.07631943375903985 ; 0.01208312017345124\n",
      "6999 ; 1:44:19.539827 ; 0.0757251035862266 ; 0.011974694984184365\n",
      "7499 ; 1:51:04.838778 ; 0.07614348579810641 ; 0.012043110539303566\n",
      "7999 ; 1:57:31.313719 ; 0.07563445430678835 ; 0.011967164474334883\n",
      "8499 ; 2:03:44.522557 ; 0.07600894222849747 ; 0.01204482314992635\n",
      "8999 ; 2:08:46.717473 ; 0.07600844538282031 ; 0.012055837563451776\n",
      "9499 ; 2:14:16.037064 ; 0.07558690388461943 ; 0.011969459540559464\n",
      "9999 ; 2:19:20.615571 ; 0.07590759075907591 ; 0.012041311693874637\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.07585809056821459, 0.012021391677966876)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
